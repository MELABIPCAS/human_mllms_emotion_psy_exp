markdown
Step-by-Step Guide: Creating Experimental Stimuli Materials

This document outlines the process of creating facial expression stimuli for an experimental study, based on the Facial Action Coding System (FACS) and specific emotional combinations. The goal is to generate both congruent and incongruent facial expressions by splicing upper and lower facial regions associated with different emotions.

Step 1: Define Key Variables in the Experiment

The experiment includes three main variables:

1. Emotion Type – Six levels:
Happiness
Sadness
Surprise
Fear
Disgust
Anger

2. Facial Expression Emotion Splicing Type – Two types:
Congruent facial emotion splicing: Upper and lower face parts express the same emotion (e.g., happy upper + happy lower).
Incongruent facial emotion splicing: Upper and lower face parts express different emotions (e.g., sad upper + happy lower).

3. Participant Type – Four levels:
Human participants
Virtual participants generated by GPT model
Virtual participants generated by Gemma model
Virtual participants generated by Qwen model

Step 2: Select Base Emotional Expressions from FACS

To ensure high validity and recognition, all original stimulus materials are derived from Ekman’s Facial Action Coding System (FACS) manual. We selected 16 representative Facial Action Units (AUs) corresponding to the six basic emotions.
Table 2: Selected AUs and Their Facial Regions

Emotion Upper Face (AUs) Lower Face (AUs) Recognition Region
------------- ------------------------ ---------------------------- --------------------
Happiness AU6 AU12, AU28 Lower face
Sadness AU4, AU43 AU15 Full face
Surprise AU1, AU2, AU5 AU25 Upper face
Fear AU1, AU4, AU5 AU25 Upper face
Disgust AU7 AU9, AU24 Upper face
Anger — AU16, AU22, AU23 Lower face
Note:
"—” indicates no specific upper face action units were used for anger.
Recognition region refers to which part of the face is most informative for identifying the emotion.

Step 3: Generate Congruent Facial Expression Combinations

For congruent expressions, we combine the upper and lower facial parts that belong to the same emotion.

Examples:
Happy upper face (AU6) + Happy lower face (AU12, AU28)
Sad upper face (AU4, AU43) + Sad lower face (AU15)
Surprised upper face (AU1, AU2, AU5) + Surprised lower face (AU25)

These combinations maintain emotional consistency across the face.

Step 4: Generate Incongruent Facial Expression Combinations

For incongruent expressions, we pair upper and lower facial parts from different emotions, forming mixed-emotion faces.

We systematically combine each of the six emotions in pairs, ensuring a balanced number of cross-combinations as shown in Table 1 below.
Table 1: Number of Combinations for Each Emotion Pair

Happiness Sadness Surprise Fear Disgust Anger
--------------- ----------- --------- ---------- ------ --------- -------
Happiness 6 2 3 2 2 3
Sadness 2 6 3 2 2 3
Surprise 3 3 6 2 2 2
Fear 2 2 2 6 4 2
Disgust 2 2 2 4 6 2
Anger 3 3 2 2 2 6
Example combinations:
Sad upper face + Happy lower face
Surprised upper face + Angry lower face
Fearful upper face + Disgusted lower face

These combinations create emotionally ambiguous or conflicting expressions to test perception and interpretation.

Step 5: Implement Image Splicing Process

Using digital image editing tools, we perform the following steps:

1. Extract Upper and Lower Facial Regions:
From original FACS images, isolate the upper face (above the nose) and lower face (below the nose) regions.
Ensure consistent lighting, pose, and scale across all images.

2. Align and Stitch Faces:
Overlay the upper facial region of one emotion onto the lower facial region of another (or same) emotion.
Use morphing or blending techniques to ensure smooth transitions at the boundary (e.g., around the nose).

3. Preserve Natural Appearance:
Adjust brightness, contrast, and color balance to minimize visible seams.
Maintain natural skin tone and facial proportions.

4. Validate Final Images:
Have independent raters assess whether the spliced faces appear natural and convey the intended emotional configuration.
Confirm that recognition regions (as defined in Table 2) remain perceptually salient.

Step 6: Prepare Stimulus Set for Testing

After generating all required combinations:
Total stimuli include both congruent and incongruent facial expressions.
Each combination is saved in standardized format (e.g., PNG, 1024x768 pixels).
All stimuli are labeled with metadata:
Emotion type (upper/lower)
Splicing type (congruent/incongruent)
Source model (human/GPT/Gemma/Qwen)

These stimuli will be presented to participants during the experiment to investigate how humans and AI models interpret mixed emotional signals.

Summary

This procedure ensures that:
Stimuli are grounded in established psychological theory (FACS).
Emotional content is precisely controlled through AU-based splicing.
Both congruent and incongruent expressions are systematically created.
Results can be reliably compared across human and AI participants.

By following these steps, we generate a robust and scientifically valid set of experimental stimuli for studying emotional perception and recognition.